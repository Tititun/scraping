{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала импортируем необходимые модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем переменные для ссылок.  \n",
    "url - основная ссылка   \n",
    "urlstub - ссылка для прикрепления аттрибутов href."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "urlstub = 'https://www.wdl.org'\n",
    "url = 'https://www.wdl.org/ru/search/?item_type=print-photograph#1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Создаем запрос на нашу главную ссылку и делаем из ответа BeautifulSoup.  \n",
    "Извлекаем из него необходимые для нас ссылки, присваиваем списку ссылок имя titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "bs = BeautifulSoup(r.text, 'html.parser')\n",
    "titles = bs.select('a.title')\n",
    "next_page = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем две фунции.  \n",
    "title_parser берет в качестве аргумента текущую страницу:  \n",
    "   1.находит на ней все ссылки на фотографии и группирует их в лист link_list.\n",
    "   2.создает csv файл pictures_* - соответствует номеру страницы\n",
    "   3.проходится по каждой ссылке в link_list и записывает нужную нам информацию в созданный csv файл, применяя к каждой ссылке функцию picture_compile.\n",
    "Здесь мы используем ThreadPoolExecutor, чтобы обрабатывать несколько запросов сразу, что многократно ускоряет сбор информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def picture_compile(link, csv_file):\n",
    "    r = requests.get(link)\n",
    "    bs = BeautifulSoup(r.text, 'html.parser')\n",
    "    r.close()\n",
    "    picture = {}\n",
    "    picture['Название'] = bs.select_one('h1#page-title').text.strip()\n",
    "    picture['Время создания'] = bs.select_one('time[itemprop=\"dateCreated\"]').attrs['datetime']\n",
    "    picture['Период'] = bs.select_one('div#subject-date time').attrs['datetime']\n",
    "    places = bs.select('div#places ul li')\n",
    "    picture['Место'] = [place.text.strip().replace('\\n', ', ') for place in places]\n",
    "    csv_columns = ['Название', 'Время создания', 'Период', 'Место']\n",
    "    with open(csv_file, 'a', encoding='utf-8') as pic:\n",
    "        writer = csv.DictWriter(pic, fieldnames=csv_columns)\n",
    "        writer.writerow(picture)\n",
    "        \n",
    "def title_parser(title_list):\n",
    "    csv_columns = ['Название', 'Время создания', 'Период', 'Место']\n",
    "    csv_file = 'pictures_' + str(next_page) +'.csv'\n",
    "    with open(csv_file, 'a', encoding='utf-8') as pic:\n",
    "        writer = csv.DictWriter(pic, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "   \n",
    "    link_list = []\n",
    "    for title in title_list:\n",
    "        photolink = urlstub + title.attrs['href']\n",
    "        link_list.append(photolink)\n",
    "    processes = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for link in link_list:\n",
    "            processes.append(executor.submit(picture_compile, link, csv_file))\n",
    "      \n",
    "    print('Сохранена страница: ', next_page)\n",
    "    time.sleep(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем эту функцию с нашим листом ссылок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "title_parser(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим для пользователя количество сохраненных ссылок.  \n",
    "Генерируем ссылку для следующей страницы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "total_saved = len(titles)\n",
    "print('сохранено ' + str(total_saved) + ' фото')\n",
    "next_page += 1\n",
    "next_stub = 'https://www.wdl.org/ru/search/?item_type=print-photograph&page='\n",
    "next_link = next_stub +str(next_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше продолжаем загружать страницы и переходить по ссылкам с фотографиями, извлекая необходимую информацию.\n",
    "Страницы будут загружаться до тех пор, пока не останется новых страниц.\n",
    "Если хотим загрузить меньше страниц, то раскомменчиваем фрагмент и указываем нужное число страниц (на одну страницу приходится 42 фото):\n",
    "if next_page == 6:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;   print('Сохранено пять страниц, хватит')  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;   break  \n",
    "Если этих строчек нет, то будут сохранены данные обо всех фотографиях на примерно 200 страницах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    r = requests.get(next_link)\n",
    "    if r.status_code != 200:\n",
    "        print('Следующей страницы не существует')\n",
    "        break\n",
    "    bs = BeautifulSoup(r.text, 'html.parser')\n",
    "    titles = bs.select('a.title')\n",
    "    title_parser(titles)\n",
    "    total_saved += len(titles)\n",
    "    print('сохранено ' + str(total_saved) + ' фото')\n",
    "\n",
    "    next_page += 1\n",
    "#     if next_page == 6:\n",
    "#         print('Сохранено пять страниц, хватит')\n",
    "#         break\n",
    "    next_link = next_stub + str(next_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединяем все собранные csv файлы в один (называем его здесь pictures.csv).\n",
    "В первом файле записываем всё, включая заголовок, в остальных файлах заголовок пропускаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = open('pictures.csv', 'a', encoding='utf8', newline='')\n",
    "writer = csv.writer(total)\n",
    "\n",
    "with open('pictures_1.csv', encoding='utf8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        if line:\n",
    "            writer.writerow(line)\n",
    "\n",
    "for i in range(2, next_page):\n",
    "    with open('pictures_'+str(i)+'.csv', encoding='utf8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) \n",
    "        for line in reader:\n",
    "            if line and line!='':\n",
    "                writer.writerow(line)\n",
    "    print(i, sep=', ')\n",
    "\n",
    "total.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
